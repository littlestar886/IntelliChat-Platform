import os
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification
import random

class TextClassifier:
    """åŸºäºBERTçš„æ–‡æœ¬åˆ†ç±»å™¨"""
    
    def __init__(self, model_path, num_labels=2, device=None):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.model_path = model_path
        self.num_labels = num_labels
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = None
        self.model = None
        
    def load_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = ['config.json', 'model.safetensors', 'vocab.txt']
            missing_files = [f for f in required_files if not os.path.exists(os.path.join(self.model_path, f))]
            
            if missing_files:
                print(f"æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼š{missing_files}ï¼Œå¼€å§‹ä¸‹è½½bert-base-chinese...")
                os.makedirs(self.model_path, exist_ok=True)
                
                # ä¸‹è½½å¹¶ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹
                self.tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
                self.model = BertForSequenceClassification.from_pretrained(
                    "bert-base-chinese", 
                    num_labels=self.num_labels
                )
                
                self.tokenizer.save_pretrained(self.model_path)
                self.model.save_pretrained(self.model_path, safe_serialization=True)
                print(f"æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{self.model_path}")
            else:
                # åŠ è½½å·²å­˜åœ¨çš„æ¨¡å‹
                self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
                self.model = BertForSequenceClassification.from_pretrained(
                    self.model_path,
                    num_labels=self.num_labels,
                    use_safetensors=True
                )
            
            self.model.to(self.device)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ from {self.model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def train(self, questions, labels, batch_size=4, learning_rate=2e-5, epochs=5, val_size=0.2, augmenter=None, augment_times=3):
        """è®­ç»ƒæ¨¡å‹"""
        if self.model is None or self.tokenizer is None:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return False
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_q, val_q, train_lbl, val_lbl = train_test_split(
            questions, labels, test_size=val_size, random_state=42)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæ”¯æŒæ•°æ®å¢å¼ºï¼‰
        from .data_utils import prepare_data  # ä»å¤–éƒ¨æ–‡ä»¶å¯¼å…¥æ•°æ®å¤„ç†å‡½æ•°
        
        train_inputs, train_labels = prepare_data(
            self.tokenizer, train_q, train_lbl, augmenter, augment_times)
        val_inputs, val_labels = prepare_data(
            self.tokenizer, val_q, val_lbl, None, 0)  # éªŒè¯é›†ä¸å¢å¼º
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels),
            batch_size=batch_size, shuffle=True)
        
        val_loader = DataLoader(
            TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels),
            batch_size=batch_size)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        # è®­ç»ƒæ¨¡å‹
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        self._train_model(self.model, train_loader, optimizer, epochs)
        
        # è¯„ä¼°æ¨¡å‹
        accuracy = self._evaluate(self.model, val_loader)
        print(f"\nğŸ¯ éªŒè¯é›†å‡†ç¡®ç‡: {accuracy*100:.2f}%")
        
        return True
    
    def predict(self, texts, apply_post_processing=True):
        """å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹"""
        if self.model is None or self.tokenizer is None:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return []
        
        self.model.eval()
        predictions = []
        
        for text in texts:
            # é¢„å¤„ç†æ–‡æœ¬
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(self.device)
            
            # æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                outputs = self.model(**inputs)
                pred = torch.argmax(outputs.logits, dim=1).item()
            
            # åå¤„ç†è§„åˆ™
            if apply_post_processing:
                pred = self._apply_post_processing(text, pred)
            
            predictions.append(pred)
        
        return predictions
    
    def save_model(self, save_path=None):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None or self.tokenizer is None:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
            return False
        
        save_path = save_path or self.model_path
        os.makedirs(save_path, exist_ok=True)
        
        try:
            self.tokenizer.save_pretrained(save_path)
            self.model.save_pretrained(save_path, safe_serialization=True)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    def _train_model(self, model, dataloader, optimizer, epochs=5):
        """æ¨¡å‹è®­ç»ƒå†…éƒ¨å‡½æ•°"""
        model.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in dataloader:
                batch = [item.to(self.device) for item in batch]
                optimizer.zero_grad()
                input_ids, attention_mask, labels = batch
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}")
    
    def _evaluate(self, model, dataloader):
        """æ¨¡å‹è¯„ä¼°å†…éƒ¨å‡½æ•°"""
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for batch in dataloader:
                batch = [item.to(self.device) for item in batch]
                input_ids, attention_mask, labels = batch
                outputs = model(input_ids, attention_mask=attention_mask)
                _, preds = torch.max(outputs.logits, 1)
                total += labels.size(0)
                correct += (preds == labels).sum().item()
        return correct / total
    
    def _apply_post_processing(self, text, pred):
        """åº”ç”¨åå¤„ç†è§„åˆ™è°ƒæ•´é¢„æµ‹ç»“æœ"""
        # ç¤ºä¾‹è§„åˆ™ï¼šåŒ…å«ç‰¹å®šå…³é”®è¯çš„é—®é¢˜å¼ºåˆ¶åˆ†ç±»ä¸º1
        if any(kw in text for kw in ["å¤©æ°”", "æ¸©åº¦", "ä¸‹é›¨", "æ°”æ¸©", "æœ€æ–°æƒ…å†µ", "æœ€è¿‘", "ç›®å‰"]):
            return 1
        return pred